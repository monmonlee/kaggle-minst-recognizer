import cv2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import seaborn as sns

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class MNISTPreprocessor:
    """MNIST è³‡æ–™å‰è™•ç†èˆ‡ EDA"""
    
    def __init__(self, train_path, test_path=None):
        """
        è¼‰å…¥è³‡æ–™
        
        Parameters:
        -----------
        train_path : str
            è¨“ç·´è³‡æ–™è·¯å¾‘ï¼ˆæœ‰ labelï¼‰
        test_path : str, optional
            æ¸¬è©¦è³‡æ–™è·¯å¾‘ï¼ˆç„¡ labelï¼Œç”¨æ–¼ Kaggle æäº¤ï¼‰
        """
        print("ğŸ“‚ è¼‰å…¥è¨“ç·´è³‡æ–™ä¸­...")
        self.train_df = pd.read_csv(train_path)
        self.train_labels = self.train_df.iloc[:, 0].values
        self.train_images = self.train_df.iloc[:, 1:].values
        print(f"âœ“ è¨“ç·´è³‡æ–™è¼‰å…¥å®Œæˆï¼š{len(self.train_df)} ç­†")
        
        # è¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼ˆå¦‚æœæœ‰æä¾›ï¼‰
        if test_path:
            print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦è³‡æ–™ä¸­...")
            self.test_df = pd.read_csv(test_path)
            self.test_images = self.test_df.values  # æ¸¬è©¦è³‡æ–™æ²’æœ‰ label
            print(f"âœ“ æ¸¬è©¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š{len(self.test_df)} ç­†")
        else:
            self.test_df = None
            self.test_images = None
    
    # ==================== EDA åŠŸèƒ½ ====================
    
    def check_basic_info(self):
        """åŸºæœ¬è³‡è¨Šæª¢æŸ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š è³‡æ–™åŸºæœ¬è³‡è¨Š")
        print("="*60)
        print(f"è¨“ç·´è³‡æ–™ç­†æ•¸ï¼š{len(self.train_df)}")
        print(f"ç‰¹å¾µæ•¸é‡ï¼š{self.train_images.shape[1]} (æ‡‰ç‚º 784)")
        print(f"æ¨™ç±¤ç¯„åœï¼š{self.train_labels.min()} ~ {self.train_labels.max()}")
        print(f"åƒç´ å€¼ç¯„åœï¼š{self.train_images.min()} ~ {self.train_images.max()}")
        
        if self.test_images is not None:
            print(f"\næ¸¬è©¦è³‡æ–™ç­†æ•¸ï¼š{len(self.test_df)}")
            print(f"æ¸¬è©¦è³‡æ–™åƒç´ å€¼ç¯„åœï¼š{self.test_images.min()} ~ {self.test_images.max()}")
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        train_missing = self.train_df.isnull().sum().sum()
        print(f"\nè¨“ç·´è³‡æ–™ç¼ºå¤±å€¼ï¼š{train_missing}")
        
        if self.test_images is not None:
            test_missing = self.test_df.isnull().sum().sum()
            print(f"æ¸¬è©¦è³‡æ–™ç¼ºå¤±å€¼ï¼š{test_missing}")
        
        # è¨˜æ†¶é«”ä½¿ç”¨
        train_memory = self.train_df.memory_usage(deep=True).sum() / 1024**2
        print(f"\nè¨“ç·´è³‡æ–™è¨˜æ†¶é«”ä½¿ç”¨ï¼š{train_memory:.2f} MB")
        
        if self.test_images is not None:
            test_memory = self.test_df.memory_usage(deep=True).sum() / 1024**2
            print(f"æ¸¬è©¦è³‡æ–™è¨˜æ†¶é«”ä½¿ç”¨ï¼š{test_memory:.2f} MB")
    
    def check_label_distribution(self):
        """æª¢æŸ¥æ¨™ç±¤åˆ†ä½ˆï¼ˆé¡åˆ¥å¹³è¡¡ï¼‰"""
        print("\n" + "="*60)
        print("ğŸ”¢ æ¨™ç±¤åˆ†ä½ˆåˆ†æ")
        print("="*60)
        
        # çµ±è¨ˆå„æ•¸å­—å‡ºç¾æ¬¡æ•¸
        label_counts = pd.Series(self.train_labels).value_counts().sort_index()
        print(label_counts)
        
        # è¨ˆç®—ä¸å¹³è¡¡ç¨‹åº¦
        max_count = label_counts.max()
        min_count = label_counts.min()
        imbalance_ratio = max_count / min_count
        print(f"\nä¸å¹³è¡¡æ¯”ä¾‹ï¼š{imbalance_ratio:.2f} (ç†æƒ³å€¼æ¥è¿‘ 1.0)")
        
        if imbalance_ratio > 1.5:
            print("âš ï¸  è­¦å‘Šï¼šè³‡æ–™ä¸å¹³è¡¡ï¼Œè€ƒæ…®ä½¿ç”¨ class_weight æˆ–é‡æ¡æ¨£")
        else:
            print("âœ“ è³‡æ–™åˆ†ä½ˆå‡è¡¡")
        
        # è¦–è¦ºåŒ–
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # é•·æ¢åœ–
        label_counts.plot(kind='bar', ax=axes[0], color='steelblue')
        axes[0].set_title('å„æ•¸å­—å‡ºç¾æ¬¡æ•¸', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('æ•¸å­—')
        axes[0].set_ylabel('æ•¸é‡')
        axes[0].grid(axis='y', alpha=0.3)
        
        # åœ“é¤…åœ–
        axes[1].pie(label_counts, labels=label_counts.index, autopct='%1.1f%%',
                    startangle=90, colors=plt.cm.tab10.colors)
        axes[1].set_title('æ•¸å­—æ¯”ä¾‹', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        return label_counts
    
    def check_pixel_statistics(self):
        """åƒç´ å€¼çµ±è¨ˆåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¨ åƒç´ å€¼çµ±è¨ˆåˆ†æ")
        print("="*60)
        
        # æ•´é«”çµ±è¨ˆ
        print("ã€è¨“ç·´è³‡æ–™ã€‘")
        print(f"å¹³å‡åƒç´ å€¼ï¼š{self.train_images.mean():.2f}")
        print(f"åƒç´ å€¼æ¨™æº–å·®ï¼š{self.train_images.std():.2f}")
        print(f"éé›¶åƒç´ æ¯”ä¾‹ï¼š{(self.train_images > 0).sum() / self.train_images.size * 100:.2f}%")
        
        if self.test_images is not None:
            print("\nã€æ¸¬è©¦è³‡æ–™ã€‘")
            print(f"å¹³å‡åƒç´ å€¼ï¼š{self.test_images.mean():.2f}")
            print(f"åƒç´ å€¼æ¨™æº–å·®ï¼š{self.test_images.std():.2f}")
            print(f"éé›¶åƒç´ æ¯”ä¾‹ï¼š{(self.test_images > 0).sum() / self.test_images.size * 100:.2f}%")
        
        # è¦–è¦ºåŒ–
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # åƒç´ å€¼åˆ†ä½ˆç›´æ–¹åœ–
        axes[0].hist(self.train_images.flatten(), bins=50, color='steelblue', alpha=0.7, label='Train')
        if self.test_images is not None:
            axes[0].hist(self.test_images.flatten(), bins=50, color='orange', alpha=0.5, label='Test')
        axes[0].set_title('åƒç´ å€¼åˆ†ä½ˆ', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('åƒç´ å€¼')
        axes[0].set_ylabel('é »ç‡')
        axes[0].set_yscale('log')
        axes[0].legend()
        axes[0].grid(alpha=0.3)
        
        # å¹³å‡åƒç´ åœ–ï¼ˆçœ‹å“ªäº›ä½ç½®å¸¸æœ‰ç­†ç•«ï¼‰
        mean_image = self.train_images.mean(axis=0).reshape(28, 28)
        im = axes[1].imshow(mean_image, cmap='hot')
        axes[1].set_title('å¹³å‡åƒç´ ç†±åŠ›åœ–ï¼ˆç­†ç•«é›†ä¸­å€åŸŸï¼‰', fontsize=14, fontweight='bold')
        axes[1].axis('off')
        plt.colorbar(im, ax=axes[1])
        
        plt.tight_layout()
        plt.show()
    
    def visualize_samples_by_label(self, samples_per_label=5):
        """æ¯å€‹æ•¸å­—é¡¯ç¤ºå¤šå€‹ç¯„ä¾‹"""
        print(f"\nğŸ“¸ é¡¯ç¤ºæ¯å€‹æ•¸å­—çš„ {samples_per_label} å€‹ç¯„ä¾‹...")
        
        fig, axes = plt.subplots(10, samples_per_label, figsize=(15, 20))
        fig.suptitle('æ¯å€‹æ•¸å­—çš„ç¯„ä¾‹', fontsize=16, fontweight='bold')
        
        for digit in range(10):
            # æ‰¾å‡ºè©²æ•¸å­—çš„æ‰€æœ‰ç´¢å¼•
            digit_indices = np.where(self.train_labels == digit)[0]
            
            # éš¨æ©Ÿé¸æ“‡ç¯„ä¾‹
            selected_indices = np.random.choice(digit_indices, 
                                               size=min(samples_per_label, len(digit_indices)),
                                               replace=False)
            
            for i, idx in enumerate(selected_indices):
                image = self.train_images[idx].reshape(28, 28)
                axes[digit, i].imshow(image, cmap='gray')
                axes[digit, i].axis('off')
                if i == 0:
                    axes[digit, i].set_ylabel(f'æ•¸å­— {digit}', fontsize=12)
        
        plt.tight_layout()
        plt.show()
    
    def detect_problematic_images(self, threshold=10):
        """åµæ¸¬å¯èƒ½æœ‰å•é¡Œçš„åœ–ç‰‡ï¼ˆå¤ªæš—æˆ–å¤ªäº®ï¼‰"""
        print(f"\nğŸ” åµæ¸¬å•é¡Œåœ–ç‰‡ï¼ˆå¹³å‡åƒç´ å€¼ < {threshold}ï¼‰...")
        
        # è¨ˆç®—æ¯å¼µåœ–çš„å¹³å‡åƒç´ å€¼
        mean_pixels = self.train_images.mean(axis=1)
        
        # æ‰¾å‡ºå¤ªæš—çš„åœ–ï¼ˆå¯èƒ½æ˜¯ç©ºç™½ï¼‰
        dark_images = np.where(mean_pixels < threshold)[0]
        
        print(f"âœ“ æ‰¾åˆ° {len(dark_images)} å¼µç–‘ä¼¼å•é¡Œåœ–ç‰‡")
        
        if len(dark_images) > 0:
            # é¡¯ç¤ºå‰ 10 å¼µ
            num_show = min(10, len(dark_images))
            fig, axes = plt.subplots(2, 5, figsize=(15, 6))
            fig.suptitle('ç–‘ä¼¼å•é¡Œåœ–ç‰‡', fontsize=16, fontweight='bold')
            
            for i in range(num_show):
                idx = dark_images[i]
                row, col = i // 5, i % 5
                image = self.train_images[idx].reshape(28, 28)
                axes[row, col].imshow(image, cmap='gray')
                axes[row, col].set_title(f'Label: {self.train_labels[idx]}\nMean: {mean_pixels[idx]:.1f}')
                axes[row, col].axis('off')
            
            plt.tight_layout()
            plt.show()
        
        return dark_images
    
    # ==================== æ­£è¦åŒ–åŠŸèƒ½ ====================
    
    def normalize_data(self, images, method='minmax'):
        """
        æ­£è¦åŒ–è³‡æ–™
        
        Parameters:
        -----------
        images : numpy array
            è¦æ­£è¦åŒ–çš„å½±åƒè³‡æ–™
        method : str
            'minmax' : ç¸®æ”¾åˆ° [0, 1]
            'standardize' : æ¨™æº–åŒ– (mean=0, std=1)
        """
        if method == 'minmax':
            # Min-Max æ­£è¦åŒ–ï¼šé™¤ä»¥ 255
            normalized = images.astype(np.float32) / 255.0
        elif method == 'standardize':
            # æ¨™æº–åŒ–ï¼š(x - mean) / std
            mean = images.mean()
            std = images.std()
            normalized = (images.astype(np.float32) - mean) / std
        else:
            raise ValueError("method å¿…é ˆæ˜¯ 'minmax' æˆ– 'standardize'")
        
        return normalized
    
    def prepare_for_cnn(self, validation_size=0.2, normalize_method='minmax', random_state=42):
        """
        æº–å‚™ CNN è¨“ç·´è³‡æ–™
        
        Parameters:
        -----------
        validation_size : float
            é©—è­‰é›†æ¯”ä¾‹ï¼ˆå¾è¨“ç·´é›†åˆ‡å‡ºï¼‰
        normalize_method : str
            æ­£è¦åŒ–æ–¹æ³•
        random_state : int
            éš¨æ©Ÿç¨®å­
        
        Returns:
        --------
        X_train, X_val, y_train, y_val, X_test
        """
        print("\n" + "="*60)
        print("ğŸš€ æº–å‚™ CNN è¨“ç·´è³‡æ–™")
        print("="*60)
        
        # 1. æ­£è¦åŒ–è¨“ç·´è³‡æ–™
        print(f"\nğŸ”§ æ­£è¦åŒ–è¨“ç·´è³‡æ–™ï¼ˆæ–¹æ³•ï¼š{normalize_method}ï¼‰...")
        normalized_train = self.normalize_data(self.train_images, method=normalize_method)
        print(f"âœ“ è¨“ç·´è³‡æ–™æ­£è¦åŒ–å®Œæˆï¼Œç¯„åœï¼š[{normalized_train.min():.3f}, {normalized_train.max():.3f}]")
        
        # 2. Reshape æˆ CNN è¼¸å…¥æ ¼å¼ (n_samples, 28, 28, 1)
        reshaped_train = normalized_train.reshape(-1, 28, 28, 1)
        print(f"âœ“ è¨“ç·´è³‡æ–™ Reshape å®Œæˆï¼Œå½¢ç‹€ï¼š{reshaped_train.shape}")
        
        # 3. åˆ‡åˆ†è¨“ç·´é›†èˆ‡é©—è­‰é›†
        X_train, X_val, y_train, y_val = train_test_split(
            reshaped_train, self.train_labels, 
            test_size=validation_size, 
            random_state=random_state,
            stratify=self.train_labels  # ç¢ºä¿è¨“ç·´é›†å’Œé©—è­‰é›†çš„æ¨™ç±¤åˆ†ä½ˆä¸€è‡´
        )
        
        print(f"\nâœ“ è³‡æ–™åˆ‡åˆ†å®Œæˆ")
        print(f"  - è¨“ç·´é›†ï¼š{len(X_train)} ç­† ({(1-validation_size)*100:.0f}%)")
        print(f"  - é©—è­‰é›†ï¼š{len(X_val)} ç­† ({validation_size*100:.0f}%)")
        
        # 4. è™•ç†æ¸¬è©¦è³‡æ–™ï¼ˆå¦‚æœæœ‰ï¼‰
        X_test = None
        if self.test_images is not None:
            print(f"\nğŸ”§ æ­£è¦åŒ–æ¸¬è©¦è³‡æ–™...")
            normalized_test = self.normalize_data(self.test_images, method=normalize_method)
            X_test = normalized_test.reshape(-1, 28, 28, 1)
            print(f"âœ“ æ¸¬è©¦è³‡æ–™è™•ç†å®Œæˆ")
            print(f"  - æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
        
        # é¡¯ç¤ºæœ€çµ‚å½¢ç‹€
        print(f"\nğŸ“Š æœ€çµ‚è³‡æ–™å½¢ç‹€ç¢ºèªï¼š")
        print(f"  X_train.shape: {X_train.shape}")
        print(f"  X_val.shape: {X_val.shape}")
        print(f"  y_train.shape: {y_train.shape}")
        print(f"  y_val.shape: {y_val.shape}")
        if X_test is not None:
            print(f"  X_test.shape: {X_test.shape}")
        
        return X_train, X_val, y_train, y_val, X_test


# ==================== ä¸»ç¨‹å¼ ====================
if __name__ == "__main__":
    TRAIN_PATH = 'train.csv'
    TEST_PATH = 'test.csv'  # Kaggle æä¾›çš„æ¸¬è©¦è³‡æ–™
    
    print("="*60)
    print("ğŸ¯ MNIST è³‡æ–™å‰è™•ç†èˆ‡ EDA å®Œæ•´æµç¨‹")
    print("="*60)
    
    # åˆå§‹åŒ–ï¼ˆè¼‰å…¥è¨“ç·´+æ¸¬è©¦è³‡æ–™ï¼‰
    preprocessor = MNISTPreprocessor(TRAIN_PATH, TEST_PATH)
    
    # ===== EDA éšæ®µ =====
    print("\n\n" + "ğŸ” é–‹å§‹ EDA åˆ†æ".center(60, "="))
    
    # 1. åŸºæœ¬è³‡è¨Š
    preprocessor.check_basic_info()
    
    # 2. æ¨™ç±¤åˆ†ä½ˆ
    label_dist = preprocessor.check_label_distribution()
    
    # 3. åƒç´ çµ±è¨ˆ
    preprocessor.check_pixel_statistics()
    
    # 4. é¡¯ç¤ºç¯„ä¾‹
    preprocessor.visualize_samples_by_label(samples_per_label=5)
    
    # 5. åµæ¸¬å•é¡Œåœ–ç‰‡
    problematic = preprocessor.detect_problematic_images(threshold=10)
    
    # ===== æ­£è¦åŒ–èˆ‡æº–å‚™è¨“ç·´è³‡æ–™ =====
    print("\n\n" + "ğŸš€ æº–å‚™è¨“ç·´è³‡æ–™".center(60, "="))
    
    X_train, X_val, y_train, y_val, X_test = preprocessor.prepare_for_cnn(
        validation_size=0.2,  # å¾è¨“ç·´é›†åˆ‡ 20% ç•¶é©—è­‰é›†
        normalize_method='minmax',
        random_state=42
    )
    
    # å„²å­˜è™•ç†å¾Œçš„è³‡æ–™
    print("\nğŸ’¾ å„²å­˜è™•ç†å¾Œçš„è³‡æ–™...")
    np.save('X_train.npy', X_train)
    np.save('X_val.npy', X_val)
    np.save('y_train.npy', y_train)
    np.save('y_val.npy', y_val)
    if X_test is not None:
        np.save('X_test.npy', X_test)
    print("âœ“ è³‡æ–™å·²å„²å­˜")
    
    print("\n" + "="*60)
    print("âœ… å‰è™•ç†å®Œæˆï¼è³‡æ–™åˆ‡åˆ†ç­–ç•¥ï¼š")
    print("="*60)
    print(f"ğŸ“Œ Train (80%)ï¼šç”¨æ–¼è¨“ç·´æ¨¡å‹")
    print(f"ğŸ“Œ Validation (20%)ï¼šç”¨æ–¼èª¿æ•´è¶…åƒæ•¸ã€ç›£æ§éæ“¬åˆ")
    print(f"ğŸ“Œ Test (Kaggle)ï¼šç”¨æ–¼æœ€çµ‚æäº¤é æ¸¬çµæœ")
    print("="*60)