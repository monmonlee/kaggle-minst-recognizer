import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
from datetime import datetime
from sklearn.metrics import accuracy_score

# è¨­å®šéš¨æ©Ÿç¨®å­
np.random.seed(42)
tf.random.set_seed(42)

class MNISTCNNWithTTA:
    """MNIST CNN è¨“ç·´å™¨ with Test Time Augmentation"""
    
    def __init__(self, model_name="MNIST_CNN_TTA"):
        """åˆå§‹åŒ–è¨“ç·´å™¨"""
        self.model_name = model_name
        self.model = None
        self.history = None
        
    def load_preprocessed_data(self):
        """è¼‰å…¥å·²å‰è™•ç†çš„è³‡æ–™"""
        print("ğŸ“‚ è¼‰å…¥å‰è™•ç†è³‡æ–™...")
        
        self.X_train = np.load('X_train.npy')
        self.X_val = np.load('X_val.npy')
        self.y_train = np.load('y_train.npy')
        self.y_val = np.load('y_val.npy')
        self.X_test = np.load('X_test.npy')
        
        print(f"âœ“ è³‡æ–™è¼‰å…¥å®Œæˆ")
        print(f"  X_train: {self.X_train.shape}")
        print(f"  X_val: {self.X_val.shape}")
        print(f"  X_test: {self.X_test.shape}")
        
        # è½‰æ›æ¨™ç±¤ç‚º One-Hot Encoding
        self.y_train_categorical = to_categorical(self.y_train, 10)
        self.y_val_categorical = to_categorical(self.y_val, 10)
        
        print(f"âœ“ æ¨™ç±¤è½‰æ›å®Œæˆï¼ˆOne-Hot Encodingï¼‰")
        
    def create_data_augmentation(self):
        """å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨ï¼ˆåŠ å…¥ Elastic Transform æ¦‚å¿µï¼‰"""
        print("\nğŸ”„ å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨...")
        
        # è¨“ç·´ç”¨ï¼šè¼ƒæ¿€é€²çš„å¢å¼·
        self.train_datagen = ImageDataGenerator(
            rotation_range=15,           # æ—‹è½‰ Â±15 åº¦
            width_shift_range=0.15,      # æ°´å¹³å¹³ç§» 15%
            height_shift_range=0.15,     # å‚ç›´å¹³ç§» 15%
            zoom_range=0.15,             # ç¸®æ”¾ Â±15%
            shear_range=0.15,            # å‰ªåˆ‡è®Šæ›ï¼ˆæ¨¡æ“¬ elastic transformï¼‰
            fill_mode='nearest'
        )
        
        # TTA ç”¨ï¼šæº«å’Œçš„å¢å¼·ï¼ˆé æ¸¬æ™‚ä½¿ç”¨ï¼‰
        self.tta_datagen = ImageDataGenerator(
            rotation_range=10,
            width_shift_range=0.1,
            height_shift_range=0.1,
            zoom_range=0.1,
            fill_mode='nearest'
        )
        
        print("âœ“ è³‡æ–™å¢å¼·ç”Ÿæˆå™¨å»ºç«‹å®Œæˆ")
        
    def build_improved_cnn(self):
        """å»ºç«‹æ”¹é€²ç‰ˆ CNNï¼ˆåŠ å…¥ BatchNormalizationï¼‰"""
        print(f"\nğŸ—ï¸  å»ºç«‹æ”¹é€²ç‰ˆ CNN æ¨¡å‹...")
        
        model = models.Sequential([
            # Block 1
            layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(32, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 2
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 3
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Dense layers
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ], name='Improved_CNN_BN')
        
        self.model = model
        
        # é¡¯ç¤ºæ¨¡å‹æ¶æ§‹
        print("\nğŸ“Š æ¨¡å‹æ¶æ§‹æ‘˜è¦ï¼š")
        model.summary()
        
        trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])
        print(f"\nâœ“ å¯è¨“ç·´åƒæ•¸æ•¸é‡ï¼š{trainable_params:,}")
        
        return model
    
    def compile_model(self, learning_rate=0.001):
        """ç·¨è­¯æ¨¡å‹"""
        print(f"\nâš™ï¸  ç·¨è­¯æ¨¡å‹ï¼ˆå­¸ç¿’ç‡ï¼š{learning_rate}ï¼‰...")
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("âœ“ æ¨¡å‹ç·¨è­¯å®Œæˆ")
    
    def train_with_augmentation(self, epochs=30, batch_size=64):
        """ä½¿ç”¨è³‡æ–™å¢å¼·è¨“ç·´æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨è³‡æ–™å¢å¼·ï¼‰")
        print("="*60)
        print(f"è¨“ç·´åƒæ•¸ï¼š")
        print(f"  - Epochs: {epochs}")
        print(f"  - Batch Size: {batch_size}")
        print(f"  - è¨“ç·´æ¨£æœ¬æ•¸: {len(self.X_train)}")
        print(f"  - é©—è­‰æ¨£æœ¬æ•¸: {len(self.X_val)}")
        print("="*60)
        
        # è¨­å®šå›èª¿å‡½æ•¸
        early_stopping = callbacks.EarlyStopping(
            monitor='val_loss',
            patience=7,
            restore_best_weights=True,
            verbose=1
        )
        
        checkpoint = callbacks.ModelCheckpoint(
            f'{self.model_name}_best.keras',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
        
        reduce_lr = callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=4,
            min_lr=1e-7,
            verbose=1
        )
        
        # ä½¿ç”¨è³‡æ–™å¢å¼·è¨“ç·´
        start_time = datetime.now()
        
        self.history = self.model.fit(
            self.train_datagen.flow(self.X_train, self.y_train_categorical, 
                                   batch_size=batch_size),
            steps_per_epoch=len(self.X_train) // batch_size,
            epochs=epochs,
            validation_data=(self.X_val, self.y_val_categorical),
            callbacks=[early_stopping, checkpoint, reduce_lr],
            verbose=1
        )
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        print("\n" + "="*60)
        print(f"âœ… è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚ï¼š{training_time:.2f} ç§’")
        print("="*60)
        
        return self.history
    
    def evaluate_on_validation(self):
        """åœ¨é©—è­‰é›†ä¸Šè©•ä¼°ï¼ˆç„¡ TTAï¼‰"""
        print("\nğŸ“ˆ è©•ä¼°æ¨¡å‹è¡¨ç¾ï¼ˆé©—è­‰é›†ï¼Œç„¡ TTAï¼‰...")
        
        val_loss, val_accuracy = self.model.evaluate(
            self.X_val, self.y_val_categorical, 
            verbose=0
        )
        
        print(f"é©—è­‰é›†æº–ç¢ºç‡ï¼š{val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
        
        return val_accuracy
    
    def predict_with_tta(self, X, n_augmentations=10):
        """
        ä½¿ç”¨ Test Time Augmentation é€²è¡Œé æ¸¬
        
        Parameters:
        -----------
        X : ndarray
            è¼¸å…¥å½±åƒ
        n_augmentations : int
            æ¯å¼µåœ–ç‰‡å¢å¼·çš„æ¬¡æ•¸
        
        Returns:
        --------
        predictions : ndarray
            å¹³å‡å¾Œçš„é æ¸¬æ©Ÿç‡
        """
        print(f"\nğŸ”® ä½¿ç”¨ TTA é€²è¡Œé æ¸¬ï¼ˆæ¯å¼µåœ–ç‰‡ {n_augmentations} æ¬¡å¢å¼·ï¼‰...")
        
        # åŸå§‹é æ¸¬
        predictions = self.model.predict(X, verbose=0)
        
        # é€²è¡Œå¤šæ¬¡å¢å¼·é æ¸¬ä¸¦å¹³å‡
        for i in range(n_augmentations - 1):
            # ç”Ÿæˆå¢å¼·ç‰ˆæœ¬
            aug_generator = self.tta_datagen.flow(X, batch_size=len(X), shuffle=False)
            X_aug = next(aug_generator)
            
            # é æ¸¬
            aug_predictions = self.model.predict(X_aug, verbose=0)
            
            # ç´¯åŠ 
            predictions += aug_predictions
            
            if (i + 1) % 3 == 0:
                print(f"  é€²åº¦ï¼š{i + 2}/{n_augmentations}")
        
        # å¹³å‡
        predictions = predictions / n_augmentations
        
        print(f"âœ“ TTA é æ¸¬å®Œæˆ")
        
        return predictions
    
    def evaluate_with_tta(self, n_augmentations=10):
        """åœ¨é©—è­‰é›†ä¸Šä½¿ç”¨ TTA è©•ä¼°"""
        print("\nğŸ“ˆ è©•ä¼°æ¨¡å‹è¡¨ç¾ï¼ˆé©—è­‰é›†ï¼Œä½¿ç”¨ TTAï¼‰...")
        
        # ä½¿ç”¨ TTA é æ¸¬
        val_predictions = self.predict_with_tta(self.X_val, n_augmentations)
        val_pred_labels = np.argmax(val_predictions, axis=1)
        
        # è¨ˆç®—æº–ç¢ºç‡
        tta_accuracy = accuracy_score(self.y_val, val_pred_labels)
        
        print(f"é©—è­‰é›†æº–ç¢ºç‡ï¼ˆTTAï¼‰ï¼š{tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)")
        
        # æ¯”è¼ƒæ”¹å–„
        no_tta_accuracy = self.evaluate_on_validation()
        improvement = (tta_accuracy - no_tta_accuracy) * 100
        print(f"\nâœ¨ TTA æ”¹å–„ï¼š+{improvement:.2f}%")
        
        return tta_accuracy
    
    def predict_test_set_with_tta(self, n_augmentations=10, output_path='submission_tta.csv'):
        """ä½¿ç”¨ TTA é æ¸¬æ¸¬è©¦é›†ä¸¦ç”¢ç”Ÿæäº¤æª”æ¡ˆ"""
        print("\n" + "="*60)
        print("ğŸ¯ é æ¸¬æ¸¬è©¦é›†ï¼ˆä½¿ç”¨ TTAï¼‰")
        print("="*60)
        
        # ä½¿ç”¨ TTA é æ¸¬
        test_predictions = self.predict_with_tta(self.X_test, n_augmentations)
        test_labels = np.argmax(test_predictions, axis=1)
        
        # ç”¢ç”Ÿæäº¤æª”æ¡ˆ
        submission = pd.DataFrame({
            'ImageId': range(1, len(test_labels) + 1),
            'Label': test_labels
        })
        
        submission.to_csv(output_path, index=False)
        
        print(f"\nâœ“ é æ¸¬å®Œæˆï¼")
        print(f"âœ“ æäº¤æª”æ¡ˆå·²å„²å­˜ï¼š{output_path}")
        print(f"âœ“ ç¸½å…±é æ¸¬ï¼š{len(test_labels)} ç­†è³‡æ–™")
        print(f"\né æ¸¬æ¨™ç±¤åˆ†ä½ˆï¼š")
        print(submission['Label'].value_counts().sort_index())
        
        return submission
    
    def save_model(self, filepath=None):
        """å„²å­˜æ¨¡å‹"""
        if filepath is None:
            filepath = f'{self.model_name}_final.keras'
        
        self.model.save(filepath)
        print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜ï¼š{filepath}")


# ==================== ä¸»ç¨‹å¼ ====================
if __name__ == "__main__":
    print("="*60)
    print("ğŸ¯ MNIST CNN + TTA è¨“ç·´æµç¨‹")
    print("="*60)
    
    # 1. åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = MNISTCNNWithTTA(model_name="MNIST_CNN_TTA")
    
    # 2. è¼‰å…¥è³‡æ–™
    trainer.load_preprocessed_data()
    
    # 3. å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨
    trainer.create_data_augmentation()
    
    # 4. å»ºç«‹æ”¹é€²ç‰ˆæ¨¡å‹
    trainer.build_improved_cnn()
    
    # 5. ç·¨è­¯æ¨¡å‹
    trainer.compile_model(learning_rate=0.001)
    
    # 6. è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨è³‡æ–™å¢å¼·ï¼‰
    history = trainer.train_with_augmentation(
        epochs=30,
        batch_size=64
    )
    
    # 7. è©•ä¼°æ¨¡å‹ï¼ˆé©—è­‰é›†ï¼‰
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹è©•ä¼°")
    print("="*60)
    
    # ç„¡ TTA çš„æº–ç¢ºç‡
    no_tta_acc = trainer.evaluate_on_validation()
    
    # ä½¿ç”¨ TTA çš„æº–ç¢ºç‡ï¼ˆåœ¨é©—è­‰é›†ä¸Šæ¸¬è©¦ï¼‰
    tta_acc = trainer.evaluate_with_tta(n_augmentations=10)
    
    # 8. é æ¸¬æ¸¬è©¦é›†ï¼ˆä½¿ç”¨ TTAï¼‰
    print("\n" + "="*60)
    print("ğŸš€ é–‹å§‹é æ¸¬æ¸¬è©¦é›†")
    print("="*60)
    
    submission = trainer.predict_test_set_with_tta(
        n_augmentations=10,  # å¯ä»¥èª¿æ•´é€™å€‹æ•¸å­—ï¼ˆ5-15 éƒ½å¯ä»¥ï¼‰
        output_path='submission_tta.csv'
    )
    
    # 9. å„²å­˜æ¨¡å‹
    trainer.save_model()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµç¨‹å®Œæˆï¼")
    print("="*60)
    print("\nç”¢ç”Ÿçš„æª”æ¡ˆï¼š")
    print("  ğŸ“„ submission_tta.csv  â† é€™æ˜¯è¦æäº¤åˆ° Kaggle çš„æª”æ¡ˆ")
    print("  ğŸ’¾ MNIST_CNN_TTA_best.keras")
    print("  ğŸ’¾ MNIST_CNN_TTA_final.keras")
    print("="*60)
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - submission_tta.csv ä½¿ç”¨äº† TTAï¼Œæº–ç¢ºç‡æ‡‰è©²æœƒæ¯”åŸæœ¬æå‡ 0.5-1%")
    print("  - å¦‚æœæƒ³è¦æ›´é«˜æº–ç¢ºç‡ï¼Œå¯ä»¥å¢åŠ  n_augmentations åˆ° 15-20")
    print("  - è¨“ç·´æ›´å¤š epochsï¼ˆä¾‹å¦‚ 40-50ï¼‰ä¹Ÿå¯èƒ½æœ‰å¹«åŠ©")
    print("="*60)