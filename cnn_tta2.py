import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
from datetime import datetime
from sklearn.metrics import accuracy_score

# è¨­å®šéš¨æ©Ÿç¨®å­
np.random.seed(42)
tf.random.set_seed(42)

class MNISTCNNWithEnhancedTTA:
    """MNIST CNN è¨“ç·´å™¨ with Enhanced TTA (å¢å¼·ç‰ˆ TTA)"""
    
    def __init__(self, model_name="MNIST_CNN_Enhanced_TTA"):
        """åˆå§‹åŒ–è¨“ç·´å™¨"""
        self.model_name = model_name
        self.model = None
        self.history = None
        
    def load_preprocessed_data(self):
        """è¼‰å…¥å·²å‰è™•ç†çš„è³‡æ–™"""
        print("ğŸ“‚ è¼‰å…¥å‰è™•ç†è³‡æ–™...")
        
        self.X_train = np.load('X_train.npy')
        self.X_val = np.load('X_val.npy')
        self.y_train = np.load('y_train.npy')
        self.y_val = np.load('y_val.npy')
        self.X_test = np.load('X_test.npy')
        
        print(f"âœ“ è³‡æ–™è¼‰å…¥å®Œæˆ")
        print(f"  X_train: {self.X_train.shape}")
        print(f"  X_val: {self.X_val.shape}")
        print(f"  X_test: {self.X_test.shape}")
        
        # è½‰æ›æ¨™ç±¤ç‚º One-Hot Encoding
        self.y_train_categorical = to_categorical(self.y_train, 10)
        self.y_val_categorical = to_categorical(self.y_val, 10)
        
        print(f"âœ“ æ¨™ç±¤è½‰æ›å®Œæˆï¼ˆOne-Hot Encodingï¼‰")
        
    def create_data_augmentation(self):
        """å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨"""
        print("\nğŸ”„ å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨...")
        
        # è¨“ç·´ç”¨ï¼šé©ä¸­çš„å¢å¼·ï¼ˆåŸºæ–¼ä½ è¡¨ç¾æœ€å¥½çš„ç‰ˆæœ¬ï¼‰
        self.train_datagen = ImageDataGenerator(
            rotation_range=15,           # æ—‹è½‰ Â±15 åº¦
            width_shift_range=0.15,      # æ°´å¹³å¹³ç§» 15%
            height_shift_range=0.15,     # å‚ç›´å¹³ç§» 15%
            zoom_range=0.15,             # ç¸®æ”¾ Â±15%
            shear_range=0.15,            # å‰ªåˆ‡è®Šæ›
            fill_mode='nearest'
        )
        
        # TTA ç”¨ï¼šæº«å’Œçš„å¢å¼·ï¼ˆé æ¸¬æ™‚ä½¿ç”¨ï¼‰
        self.tta_datagen = ImageDataGenerator(
            rotation_range=10,
            width_shift_range=0.1,
            height_shift_range=0.1,
            zoom_range=0.1,
            fill_mode='nearest'
        )
        
        print("âœ“ è³‡æ–™å¢å¼·ç”Ÿæˆå™¨å»ºç«‹å®Œæˆ")
        
    def build_improved_cnn(self):
        """å»ºç«‹æ”¹é€²ç‰ˆ CNNï¼ˆåŠ å…¥ BatchNormalizationï¼‰"""
        print(f"\nğŸ—ï¸  å»ºç«‹æ”¹é€²ç‰ˆ CNN æ¨¡å‹...")
        
        model = models.Sequential([
            # Block 1
            layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(32, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 2
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 3
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Dense layers
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ], name='Improved_CNN_Enhanced_TTA')
        
        self.model = model
        
        # é¡¯ç¤ºæ¨¡å‹æ¶æ§‹
        print("\nğŸ“Š æ¨¡å‹æ¶æ§‹æ‘˜è¦ï¼š")
        model.summary()
        
        trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])
        print(f"\nâœ“ å¯è¨“ç·´åƒæ•¸æ•¸é‡ï¼š{trainable_params:,}")
        
        return model
    
    def compile_model(self, learning_rate=0.001):
        """ç·¨è­¯æ¨¡å‹"""
        print(f"\nâš™ï¸  ç·¨è­¯æ¨¡å‹ï¼ˆå­¸ç¿’ç‡ï¼š{learning_rate}ï¼‰...")
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("âœ“ æ¨¡å‹ç·¨è­¯å®Œæˆ")
    
    def train_with_augmentation(self, epochs=30, batch_size=64):
        """ä½¿ç”¨è³‡æ–™å¢å¼·è¨“ç·´æ¨¡å‹"""
        print("\n" + "="*60)
        print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨è³‡æ–™å¢å¼·ï¼‰")
        print("="*60)
        print(f"è¨“ç·´åƒæ•¸ï¼š")
        print(f"  - Epochs: {epochs}")
        print(f"  - Batch Size: {batch_size}")
        print(f"  - è¨“ç·´æ¨£æœ¬æ•¸: {len(self.X_train)}")
        print(f"  - é©—è­‰æ¨£æœ¬æ•¸: {len(self.X_val)}")
        print("="*60)
        
        # è¨­å®šå›èª¿å‡½æ•¸
        early_stopping = callbacks.EarlyStopping(
            monitor='val_loss',
            patience=7,
            restore_best_weights=True,
            verbose=1
        )
        
        checkpoint = callbacks.ModelCheckpoint(
            f'{self.model_name}_best.keras',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
        
        reduce_lr = callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=4,
            min_lr=1e-7,
            verbose=1
        )
        
        # ä½¿ç”¨è³‡æ–™å¢å¼·è¨“ç·´
        start_time = datetime.now()
        
        self.history = self.model.fit(
            self.train_datagen.flow(self.X_train, self.y_train_categorical, 
                                   batch_size=batch_size),
            steps_per_epoch=len(self.X_train) // batch_size,
            epochs=epochs,
            validation_data=(self.X_val, self.y_val_categorical),
            callbacks=[early_stopping, checkpoint, reduce_lr],
            verbose=1
        )
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        print("\n" + "="*60)
        print(f"âœ… è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚ï¼š{training_time:.2f} ç§’")
        print("="*60)
        
        return self.history
    
    def evaluate_on_validation(self):
        """åœ¨é©—è­‰é›†ä¸Šè©•ä¼°ï¼ˆç„¡ TTAï¼‰"""
        print("\nğŸ“ˆ è©•ä¼°æ¨¡å‹è¡¨ç¾ï¼ˆé©—è­‰é›†ï¼Œç„¡ TTAï¼‰...")
        
        val_loss, val_accuracy = self.model.evaluate(
            self.X_val, self.y_val_categorical, 
            verbose=0
        )
        
        print(f"é©—è­‰é›†æº–ç¢ºç‡ï¼ˆç„¡ TTAï¼‰ï¼š{val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
        
        return val_accuracy
    
    def predict_with_enhanced_tta(self, X, n_augmentations=20):
        """
        ä½¿ç”¨å¢å¼·ç‰ˆ TTA é€²è¡Œé æ¸¬ï¼ˆæ›´å¤šæ¬¡æ•¸ï¼‰
        
        Parameters:
        -----------
        X : ndarray
            è¼¸å…¥å½±åƒ
        n_augmentations : int
            æ¯å¼µåœ–ç‰‡å¢å¼·çš„æ¬¡æ•¸ï¼ˆå»ºè­° 15-20ï¼‰
        
        Returns:
        --------
        predictions : ndarray
            å¹³å‡å¾Œçš„é æ¸¬æ©Ÿç‡
        """
        print(f"\nğŸ”® ä½¿ç”¨å¢å¼·ç‰ˆ TTA é€²è¡Œé æ¸¬ï¼ˆæ¯å¼µåœ–ç‰‡ {n_augmentations} æ¬¡å¢å¼·ï¼‰...")
        print(f"   é è¨ˆè™•ç†æ™‚é–“ï¼šç´„ {n_augmentations * 2} ç§’")
        
        # åŸå§‹é æ¸¬
        predictions = self.model.predict(X, verbose=0)
        
        # é€²è¡Œå¤šæ¬¡å¢å¼·é æ¸¬ä¸¦å¹³å‡
        for i in range(n_augmentations - 1):
            # ç”Ÿæˆå¢å¼·ç‰ˆæœ¬
            aug_generator = self.tta_datagen.flow(X, batch_size=len(X), shuffle=False)
            X_aug = next(aug_generator)
            
            # é æ¸¬
            aug_predictions = self.model.predict(X_aug, verbose=0)
            
            # ç´¯åŠ 
            predictions += aug_predictions
            
            # é¡¯ç¤ºé€²åº¦
            if (i + 1) % 5 == 0:
                progress = (i + 2) / n_augmentations * 100
                print(f"  é€²åº¦ï¼š{i + 2}/{n_augmentations} ({progress:.1f}%)")
        
        # å¹³å‡
        predictions = predictions / n_augmentations
        
        print(f"âœ“ å¢å¼·ç‰ˆ TTA é æ¸¬å®Œæˆï¼ˆå…± {n_augmentations} æ¬¡å¢å¼·ï¼‰")
        
        return predictions
    
    def evaluate_with_tta(self, n_augmentations=20):
        """åœ¨é©—è­‰é›†ä¸Šä½¿ç”¨ TTA è©•ä¼°"""
        print("\nğŸ“ˆ è©•ä¼°æ¨¡å‹è¡¨ç¾ï¼ˆé©—è­‰é›†ï¼Œä½¿ç”¨å¢å¼·ç‰ˆ TTAï¼‰...")
        
        # ä½¿ç”¨ TTA é æ¸¬
        val_predictions = self.predict_with_enhanced_tta(self.X_val, n_augmentations)
        val_pred_labels = np.argmax(val_predictions, axis=1)
        
        # è¨ˆç®—æº–ç¢ºç‡
        tta_accuracy = accuracy_score(self.y_val, val_pred_labels)
        
        print(f"é©—è­‰é›†æº–ç¢ºç‡ï¼ˆTTA x{n_augmentations}ï¼‰ï¼š{tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)")
        
        # æ¯”è¼ƒæ”¹å–„
        no_tta_accuracy = self.evaluate_on_validation()
        improvement = (tta_accuracy - no_tta_accuracy) * 100
        print(f"\nâœ¨ TTA æ”¹å–„ï¼š+{improvement:.2f}%")
        
        return tta_accuracy
    
    def predict_test_set_with_enhanced_tta(self, n_augmentations=20, output_path='submission_enhanced_tta.csv'):
        """ä½¿ç”¨å¢å¼·ç‰ˆ TTA é æ¸¬æ¸¬è©¦é›†ä¸¦ç”¢ç”Ÿæäº¤æª”æ¡ˆ"""
        print("\n" + "="*60)
        print(f"ğŸ¯ é æ¸¬æ¸¬è©¦é›†ï¼ˆä½¿ç”¨å¢å¼·ç‰ˆ TTA x{n_augmentations}ï¼‰")
        print("="*60)
        
        # ä½¿ç”¨ TTA é æ¸¬
        test_predictions = self.predict_with_enhanced_tta(self.X_test, n_augmentations)
        test_labels = np.argmax(test_predictions, axis=1)
        
        # ç”¢ç”Ÿæäº¤æª”æ¡ˆ
        submission = pd.DataFrame({
            'ImageId': range(1, len(test_labels) + 1),
            'Label': test_labels
        })
        
        submission.to_csv(output_path, index=False)
        
        print(f"\nâœ“ é æ¸¬å®Œæˆï¼")
        print(f"âœ“ æäº¤æª”æ¡ˆå·²å„²å­˜ï¼š{output_path}")
        print(f"âœ“ ç¸½å…±é æ¸¬ï¼š{len(test_labels)} ç­†è³‡æ–™")
        print(f"\né æ¸¬æ¨™ç±¤åˆ†ä½ˆï¼š")
        print(submission['Label'].value_counts().sort_index())
        
        return submission
    
    def save_model(self, filepath=None):
        """å„²å­˜æ¨¡å‹"""
        if filepath is None:
            filepath = f'{self.model_name}_final.keras'
        
        self.model.save(filepath)
        print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜ï¼š{filepath}")


# ==================== ä¸»ç¨‹å¼ ====================
if __name__ == "__main__":
    print("="*60)
    print("ğŸ¯ MNIST CNN + å¢å¼·ç‰ˆ TTA (Enhanced TTA)")
    print("="*60)
    
    # 1. åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = MNISTCNNWithEnhancedTTA(model_name="MNIST_CNN_Enhanced_TTA")
    
    # 2. è¼‰å…¥è³‡æ–™
    trainer.load_preprocessed_data()
    
    # 3. å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨
    trainer.create_data_augmentation()
    
    # 4. å»ºç«‹æ”¹é€²ç‰ˆæ¨¡å‹
    trainer.build_improved_cnn()
    
    # 5. ç·¨è­¯æ¨¡å‹
    trainer.compile_model(learning_rate=0.001)
    
    # 6. è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨è³‡æ–™å¢å¼·ï¼‰
    history = trainer.train_with_augmentation(
        epochs=30,
        batch_size=64
    )
    
    # 7. è©•ä¼°æ¨¡å‹ï¼ˆé©—è­‰é›†ï¼‰
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹è©•ä¼°")
    print("="*60)
    
    # ç„¡ TTA çš„æº–ç¢ºç‡
    no_tta_acc = trainer.evaluate_on_validation()
    
    # ä½¿ç”¨å¢å¼·ç‰ˆ TTA çš„æº–ç¢ºç‡ï¼ˆåœ¨é©—è­‰é›†ä¸Šæ¸¬è©¦ï¼‰
    print("\n" + "-"*60)
    print("âš ï¸  ä»¥ä¸‹æ˜¯åœ¨é©—è­‰é›†ä¸Šæ¸¬è©¦ä¸åŒ TTA æ¬¡æ•¸çš„æ•ˆæœ")
    print("-"*60)
    
    # æ¸¬è©¦ TTA=10ï¼ˆè·Ÿä¹‹å‰ä¸€æ¨£ï¼‰
    print("\nã€æ¸¬è©¦ 1ã€‘TTA æ¬¡æ•¸ = 10")
    tta_10_acc = trainer.evaluate_with_tta(n_augmentations=10)
    
    # æ¸¬è©¦ TTA=15
    print("\nã€æ¸¬è©¦ 2ã€‘TTA æ¬¡æ•¸ = 15")
    tta_15_acc = trainer.evaluate_with_tta(n_augmentations=15)
    
    # æ¸¬è©¦ TTA=20
    print("\nã€æ¸¬è©¦ 3ã€‘TTA æ¬¡æ•¸ = 20")
    tta_20_acc = trainer.evaluate_with_tta(n_augmentations=20)
    
    # æ¯”è¼ƒçµæœ
    print("\n" + "="*60)
    print("ğŸ“Š TTA æ¬¡æ•¸æ¯”è¼ƒçµæœ")
    print("="*60)
    print(f"ç„¡ TTAï¼š      {no_tta_acc:.4f} ({no_tta_acc*100:.2f}%)")
    print(f"TTA x10ï¼š     {tta_10_acc:.4f} ({tta_10_acc*100:.2f}%)")
    print(f"TTA x15ï¼š     {tta_15_acc:.4f} ({tta_15_acc*100:.2f}%)")
    print(f"TTA x20ï¼š     {tta_20_acc:.4f} ({tta_20_acc*100:.2f}%)")
    print("="*60)
    
    # 8. é æ¸¬æ¸¬è©¦é›†ï¼ˆä½¿ç”¨æœ€ä½³çš„ TTA æ¬¡æ•¸ï¼‰
    print("\n" + "="*60)
    print("ğŸš€ é–‹å§‹é æ¸¬æ¸¬è©¦é›†")
    print("="*60)
    print("\nğŸ’¡ æ ¹æ“šé©—è­‰é›†çµæœï¼Œé¸æ“‡ä½¿ç”¨ TTA x20")
    
    # ä½ å¯ä»¥æ ¹æ“šé©—è­‰é›†çµæœèª¿æ•´é€™å€‹æ•¸å­—
    best_tta_count = 25  # å¯æ”¹æˆ 15 æˆ– 20
    
    submission = trainer.predict_test_set_with_enhanced_tta(
        n_augmentations=best_tta_count,
        output_path='submission_enhanced_tta_best_test.csv'
    )
    
    # 9. å„²å­˜æ¨¡å‹
    trainer.save_model()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµç¨‹å®Œæˆï¼")
    print("="*60)
    print("\nç”¢ç”Ÿçš„æª”æ¡ˆï¼š")
    print("  ğŸ“„ submission_enhanced_tta.csv  â† æäº¤é€™å€‹åˆ° Kaggle")
    print("  ğŸ’¾ MNIST_CNN_Enhanced_TTA_best.keras")
    print("  ğŸ’¾ MNIST_CNN_Enhanced_TTA_final.keras")
    print("="*60)
    print("\nğŸ¯ æ”¹é€²é‡é»ï¼š")
    print("  1. âœ… åŸºæ–¼è¡¨ç¾æœ€å¥½çš„ TTA ç‰ˆæœ¬")
    print("  2. âœ… å¢åŠ  TTA æ¬¡æ•¸åˆ° 15-20 æ¬¡")
    print("  3. âœ… åœ¨é©—è­‰é›†ä¸Šæ¸¬è©¦ä¸åŒ TTA æ¬¡æ•¸çš„æ•ˆæœ")
    print("  4. âœ… é æœŸæå‡ï¼š0.99371 â†’ 0.994-0.996")
    print("="*60)
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - å¦‚æœé©—è­‰é›†ä¸Š TTA x15 å’Œ x20 å·®ä¸å¤šï¼Œç”¨ x15 å°±å¥½ï¼ˆæ›´å¿«ï¼‰")
    print("  - å¦‚æœæƒ³æ›´æ¿€é€²ï¼Œå¯ä»¥è©¦è©¦ TTA x25 æˆ– x30")
    print("  - è¨˜å¾—è§€å¯Ÿé©—è­‰é›†çš„æ”¹å–„æ˜¯å¦é£½å’Œ")
    print("="*60)