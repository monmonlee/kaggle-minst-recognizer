import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import json
from collections import Counter

print("="*70)
print("ğŸ¯ Ensemble é æ¸¬è…³æœ¬ - çµ„åˆå¤šå€‹æ¨¡å‹çš„é æ¸¬")
print("="*70)

# 1. è¼‰å…¥æ¨¡å‹è³‡è¨Š
print("\nğŸ“‚ è¼‰å…¥æ¨¡å‹è³‡è¨Š...")
with open('ensemble_models_info.json', 'r') as f:
    models_info = json.load(f)

print(f"âœ“ æ‰¾åˆ° {len(models_info)} å€‹æ¨¡å‹")
for info in models_info:
    print(f"  - {info['model_name']}: é©—è­‰é›†æº–ç¢ºç‡ {info['val_accuracy']:.4f}")

# 2. è¼‰å…¥æ¸¬è©¦è³‡æ–™
print("\nğŸ“‚ è¼‰å…¥æ¸¬è©¦è³‡æ–™...")
X_test = np.load('X_test.npy')
print(f"âœ“ æ¸¬è©¦é›†è¼‰å…¥å®Œæˆï¼š{X_test.shape}")

# 3. è¼‰å…¥æ‰€æœ‰æ¨¡å‹
print("\nğŸ’¾ è¼‰å…¥æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹...")
models = []
for info in models_info:
    model_path = f"{info['model_name']}_best.keras"
    try:
        model = load_model(model_path)
        models.append({
            'model': model,
            'name': info['model_name'],
            'val_accuracy': info['val_accuracy']
        })
        print(f"  âœ“ è¼‰å…¥ï¼š{info['model_name']}")
    except Exception as e:
        print(f"  âš ï¸  ç„¡æ³•è¼‰å…¥ {model_path}: {e}")

print(f"\nâœ“ æˆåŠŸè¼‰å…¥ {len(models)} å€‹æ¨¡å‹")

# 4. å»ºç«‹ TTA ç”Ÿæˆå™¨
print("\nğŸ”„ å»ºç«‹ TTA è³‡æ–™å¢å¼·å™¨...")
tta_datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    fill_mode='nearest'
)
print("âœ“ TTA ç”Ÿæˆå™¨å»ºç«‹å®Œæˆ")

# 5. å®šç¾© TTA é æ¸¬å‡½æ•¸
def predict_with_tta(model, X, n_augmentations=15):
    """ä½¿ç”¨ TTA é€²è¡Œé æ¸¬"""
    # åŸå§‹é æ¸¬
    predictions = model.predict(X, verbose=0)
    
    # é€²è¡Œå¤šæ¬¡å¢å¼·é æ¸¬
    for i in range(n_augmentations - 1):
        aug_generator = tta_datagen.flow(X, batch_size=len(X), shuffle=False)
        X_aug = next(aug_generator)
        aug_predictions = model.predict(X_aug, verbose=0)
        predictions += aug_predictions
    
    # å¹³å‡
    predictions = predictions / n_augmentations
    
    return predictions

# 6. ä½¿ç”¨æ¯å€‹æ¨¡å‹é€²è¡Œé æ¸¬
print("\n" + "="*70)
print("ğŸ”® é–‹å§‹ä½¿ç”¨æ¯å€‹æ¨¡å‹é€²è¡Œ TTA é æ¸¬...")
print("="*70)

all_predictions = []
all_pred_labels = []

for i, model_dict in enumerate(models, 1):
    model = model_dict['model']
    model_name = model_dict['name']
    
    print(f"\n[{i}/{len(models)}] {model_name}")
    print(f"  ä½¿ç”¨ TTA x15 é æ¸¬...")
    
    # TTA é æ¸¬
    predictions = predict_with_tta(model, X_test, n_augmentations=15)
    pred_labels = np.argmax(predictions, axis=1)
    
    all_predictions.append(predictions)
    all_pred_labels.append(pred_labels)
    
    print(f"  âœ“ é æ¸¬å®Œæˆ")

print("\nâœ“ æ‰€æœ‰æ¨¡å‹é æ¸¬å®Œæˆï¼")

# 7. Ensemble æ–¹æ³• 1ï¼šè»ŸæŠ•ç¥¨ï¼ˆæ©Ÿç‡å¹³å‡ï¼‰
print("\n" + "="*70)
print("ğŸ“Š Ensemble æ–¹æ³• 1ï¼šè»ŸæŠ•ç¥¨ï¼ˆSoft Voting - æ©Ÿç‡å¹³å‡ï¼‰")
print("="*70)

# å¹³å‡æ‰€æœ‰æ¨¡å‹çš„é æ¸¬æ©Ÿç‡
avg_predictions = np.mean(all_predictions, axis=0)
soft_voting_labels = np.argmax(avg_predictions, axis=1)

# ç”¢ç”Ÿæäº¤æª”æ¡ˆ
submission_soft = pd.DataFrame({
    'ImageId': range(1, len(soft_voting_labels) + 1),
    'Label': soft_voting_labels
})
submission_soft.to_csv('submission_ensemble_soft_voting.csv', index=False)

print("âœ“ è»ŸæŠ•ç¥¨é æ¸¬å®Œæˆ")
print(f"âœ“ å·²å„²å­˜ï¼šsubmission_ensemble_soft_voting.csv")
print(f"\né æ¸¬æ¨™ç±¤åˆ†ä½ˆï¼š")
print(submission_soft['Label'].value_counts().sort_index())

# 8. Ensemble æ–¹æ³• 2ï¼šç¡¬æŠ•ç¥¨ï¼ˆå¤šæ•¸æ±ºï¼‰
print("\n" + "="*70)
print("ğŸ“Š Ensemble æ–¹æ³• 2ï¼šç¡¬æŠ•ç¥¨ï¼ˆHard Voting - å¤šæ•¸æ±ºï¼‰")
print("="*70)

# å°æ¯å€‹æ¸¬è©¦æ¨£æœ¬ï¼Œçµ±è¨ˆå„æ¨¡å‹çš„é æ¸¬çµæœï¼Œå–æœ€å¤šçš„
hard_voting_labels = []
for i in range(len(X_test)):
    votes = [pred_labels[i] for pred_labels in all_pred_labels]
    # çµ±è¨ˆæŠ•ç¥¨ï¼Œå–æœ€å¤šçš„
    most_common = Counter(votes).most_common(1)[0][0]
    hard_voting_labels.append(most_common)

hard_voting_labels = np.array(hard_voting_labels)

# ç”¢ç”Ÿæäº¤æª”æ¡ˆ
submission_hard = pd.DataFrame({
    'ImageId': range(1, len(hard_voting_labels) + 1),
    'Label': hard_voting_labels
})
submission_hard.to_csv('submission_ensemble_hard_voting.csv', index=False)

print("âœ“ ç¡¬æŠ•ç¥¨é æ¸¬å®Œæˆ")
print(f"âœ“ å·²å„²å­˜ï¼šsubmission_ensemble_hard_voting.csv")
print(f"\né æ¸¬æ¨™ç±¤åˆ†ä½ˆï¼š")
print(submission_hard['Label'].value_counts().sort_index())

# 9. Ensemble æ–¹æ³• 3ï¼šåŠ æ¬Šè»ŸæŠ•ç¥¨ï¼ˆæ ¹æ“šé©—è­‰é›†æº–ç¢ºç‡åŠ æ¬Šï¼‰
print("\n" + "="*70)
print("ğŸ“Š Ensemble æ–¹æ³• 3ï¼šåŠ æ¬Šè»ŸæŠ•ç¥¨ï¼ˆæ ¹æ“šé©—è­‰é›†æº–ç¢ºç‡ï¼‰")
print("="*70)

# è¨ˆç®—æ¬Šé‡ï¼ˆé©—è­‰é›†æº–ç¢ºç‡ï¼‰
weights = np.array([model_dict['val_accuracy'] for model_dict in models])
weights = weights / weights.sum()  # æ­£è¦åŒ–

print("æ¨¡å‹æ¬Šé‡ï¼š")
for i, (model_dict, weight) in enumerate(zip(models, weights)):
    print(f"  {model_dict['name']}: {weight:.4f} (é©—è­‰é›†æº–ç¢ºç‡: {model_dict['val_accuracy']:.4f})")

# åŠ æ¬Šå¹³å‡
weighted_predictions = np.average(all_predictions, axis=0, weights=weights)
weighted_voting_labels = np.argmax(weighted_predictions, axis=1)

# ç”¢ç”Ÿæäº¤æª”æ¡ˆ
submission_weighted = pd.DataFrame({
    'ImageId': range(1, len(weighted_voting_labels) + 1),
    'Label': weighted_voting_labels
})
submission_weighted.to_csv('submission_ensemble_weighted_voting.csv', index=False)

print("\nâœ“ åŠ æ¬Šè»ŸæŠ•ç¥¨é æ¸¬å®Œæˆ")
print(f"âœ“ å·²å„²å­˜ï¼šsubmission_ensemble_weighted_voting.csv")
print(f"\né æ¸¬æ¨™ç±¤åˆ†ä½ˆï¼š")
print(submission_weighted['Label'].value_counts().sort_index())

# 10. æ¯”è¼ƒä¸‰ç¨®æ–¹æ³•çš„å·®ç•°
print("\n" + "="*70)
print("ğŸ” æ¯”è¼ƒä¸‰ç¨® Ensemble æ–¹æ³•")
print("="*70)

# è¨ˆç®—ä¸‰ç¨®æ–¹æ³•çš„ä¸€è‡´æ€§
soft_vs_hard = np.sum(soft_voting_labels == hard_voting_labels)
soft_vs_weighted = np.sum(soft_voting_labels == weighted_voting_labels)
hard_vs_weighted = np.sum(hard_voting_labels == weighted_voting_labels)

total = len(X_test)
print(f"\nä¸€è‡´æ€§åˆ†æï¼š")
print(f"  è»ŸæŠ•ç¥¨ vs ç¡¬æŠ•ç¥¨ï¼š{soft_vs_hard}/{total} ({soft_vs_hard/total*100:.2f}%)")
print(f"  è»ŸæŠ•ç¥¨ vs åŠ æ¬ŠæŠ•ç¥¨ï¼š{soft_vs_weighted}/{total} ({soft_vs_weighted/total*100:.2f}%)")
print(f"  ç¡¬æŠ•ç¥¨ vs åŠ æ¬ŠæŠ•ç¥¨ï¼š{hard_vs_weighted}/{total} ({hard_vs_weighted/total*100:.2f}%)")

# æ‰¾å‡ºä¸‰ç¨®æ–¹æ³•ä¸ä¸€è‡´çš„æ¨£æœ¬
disagreement = (soft_voting_labels != hard_voting_labels) | (soft_voting_labels != weighted_voting_labels)
n_disagreement = np.sum(disagreement)
print(f"\nä¸‰ç¨®æ–¹æ³•æœ‰åˆ†æ­§çš„æ¨£æœ¬æ•¸ï¼š{n_disagreement} ({n_disagreement/total*100:.2f}%)")

# 11. æœ€çµ‚ç¸½çµ
print("\n" + "="*70)
print("âœ… Ensemble é æ¸¬å®Œæˆï¼")
print("="*70)

print("\nç”¢ç”Ÿçš„æª”æ¡ˆï¼š")
print("  ğŸ“„ submission_ensemble_soft_voting.csv     â† è»ŸæŠ•ç¥¨ï¼ˆæ¨è–¦ï¼‰")
print("  ğŸ“„ submission_ensemble_hard_voting.csv     â† ç¡¬æŠ•ç¥¨")
print("  ğŸ“„ submission_ensemble_weighted_voting.csv â† åŠ æ¬Šè»ŸæŠ•ç¥¨ï¼ˆæ¨è–¦ï¼‰")

print("\nğŸ’¡ å»ºè­°æäº¤é †åºï¼š")
print("  1. å…ˆæäº¤ï¼šsubmission_ensemble_weighted_voting.csv")
print("     ï¼ˆåŠ æ¬Šè»ŸæŠ•ç¥¨é€šå¸¸æ•ˆæœæœ€å¥½ï¼‰")
print("  2. å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå†è©¦ï¼šsubmission_ensemble_soft_voting.csv")
print("  3. æœ€å¾Œè©¦ï¼šsubmission_ensemble_hard_voting.csv")

print("\nğŸ“Š é æœŸæ•ˆæœï¼š")
print("  - Ensemble é€šå¸¸æ¯”å–®ä¸€æ¨¡å‹æå‡ 0.2-0.5%")
print("  - å¦‚æœå–®ä¸€æ¨¡å‹æœ€å¥½æ˜¯ 0.995ï¼ŒEnsemble å¯èƒ½é”åˆ° 0.997-0.998")

print("="*70)