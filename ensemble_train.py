import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
from datetime import datetime
from sklearn.metrics import accuracy_score
import json

# è¨­å®šä¸åŒçš„éš¨æ©Ÿç¨®å­ä¾†è¨“ç·´ä¸åŒçš„æ¨¡å‹
np.random.seed(42)
tf.random.set_seed(42)

class EnsembleModelTrainer:
    """Ensemble æ¨¡å‹è¨“ç·´å™¨ - è¨“ç·´å¤šå€‹ä¸åŒçš„æ¨¡å‹"""
    
    def __init__(self, model_id=1, random_seed=42):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Parameters:
        -----------
        model_id : int
            æ¨¡å‹ç·¨è™Ÿ
        random_seed : int
            éš¨æ©Ÿç¨®å­ï¼ˆæ¯å€‹æ¨¡å‹ç”¨ä¸åŒçš„ç¨®å­ï¼‰
        """
        self.model_id = model_id
        self.model_name = f"MNIST_Ensemble_Model_{model_id}"
        self.random_seed = random_seed
        self.model = None
        self.history = None
        
        # è¨­å®šéš¨æ©Ÿç¨®å­
        np.random.seed(random_seed)
        tf.random.set_seed(random_seed)
        
    def load_preprocessed_data(self):
        """è¼‰å…¥å·²å‰è™•ç†çš„è³‡æ–™"""
        print(f"\nğŸ“‚ [{self.model_name}] è¼‰å…¥å‰è™•ç†è³‡æ–™...")
        
        self.X_train = np.load('X_train.npy')
        self.X_val = np.load('X_val.npy')
        self.y_train = np.load('y_train.npy')
        self.y_val = np.load('y_val.npy')
        self.X_test = np.load('X_test.npy')
        
        print(f"âœ“ è³‡æ–™è¼‰å…¥å®Œæˆ")
        
        # è½‰æ›æ¨™ç±¤ç‚º One-Hot Encoding
        self.y_train_categorical = to_categorical(self.y_train, 10)
        self.y_val_categorical = to_categorical(self.y_val, 10)
        
    def create_data_augmentation(self, augmentation_type='standard'):
        """
        å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨ï¼ˆä¸åŒæ¨¡å‹ç”¨ä¸åŒå¼·åº¦ï¼‰
        
        Parameters:
        -----------
        augmentation_type : str
            'mild' - æº«å’Œå¢å¼·
            'standard' - æ¨™æº–å¢å¼·
            'aggressive' - æ¿€é€²å¢å¼·
        """
        print(f"\nğŸ”„ [{self.model_name}] å»ºç«‹è³‡æ–™å¢å¼·ç”Ÿæˆå™¨ï¼ˆ{augmentation_type}ï¼‰...")
        
        if augmentation_type == 'mild':
            # æº«å’Œå¢å¼·
            self.train_datagen = ImageDataGenerator(
                rotation_range=10,
                width_shift_range=0.1,
                height_shift_range=0.1,
                zoom_range=0.1,
                shear_range=0.1,
                fill_mode='nearest'
            )
        elif augmentation_type == 'aggressive':
            # æ¿€é€²å¢å¼·
            self.train_datagen = ImageDataGenerator(
                rotation_range=20,
                width_shift_range=0.2,
                height_shift_range=0.2,
                zoom_range=0.2,
                shear_range=0.2,
                fill_mode='nearest'
            )
        else:  # standard
            # æ¨™æº–å¢å¼·
            self.train_datagen = ImageDataGenerator(
                rotation_range=15,
                width_shift_range=0.15,
                height_shift_range=0.15,
                zoom_range=0.15,
                shear_range=0.15,
                fill_mode='nearest'
            )
        
        print("âœ“ è³‡æ–™å¢å¼·ç”Ÿæˆå™¨å»ºç«‹å®Œæˆ")
        
    def build_cnn_model(self, architecture='standard'):
        """
        å»ºç«‹ CNN æ¨¡å‹ï¼ˆä¸åŒæ¶æ§‹ï¼‰
        
        Parameters:
        -----------
        architecture : str
            'standard' - æ¨™æº–æ¶æ§‹
            'wide' - æ›´å¯¬çš„æ¶æ§‹ï¼ˆæ›´å¤š filtersï¼‰
            'deep' - æ›´æ·±çš„æ¶æ§‹ï¼ˆæ›´å¤šå±¤ï¼‰
        """
        print(f"\nğŸ—ï¸  [{self.model_name}] å»ºç«‹ CNN æ¨¡å‹ï¼ˆ{architecture}ï¼‰...")
        
        if architecture == 'wide':
            # æ›´å¯¬çš„æ¶æ§‹ï¼ˆæ›´å¤š filtersï¼‰
            model = self._build_wide_cnn()
        elif architecture == 'deep':
            # æ›´æ·±çš„æ¶æ§‹ï¼ˆæ›´å¤šå±¤ï¼‰
            model = self._build_deep_cnn()
        else:
            # æ¨™æº–æ¶æ§‹
            model = self._build_standard_cnn()
        
        self.model = model
        
        trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])
        print(f"âœ“ å¯è¨“ç·´åƒæ•¸æ•¸é‡ï¼š{trainable_params:,}")
        
        return model
    
    def _build_standard_cnn(self):
        """æ¨™æº– CNN æ¶æ§‹"""
        model = models.Sequential([
            # Block 1
            layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(32, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 2
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 3
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Dense layers
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ], name=f'{self.model_name}_Standard')
        
        return model
    
    def _build_wide_cnn(self):
        """æ›´å¯¬çš„ CNN æ¶æ§‹ï¼ˆæ›´å¤š filtersï¼‰"""
        model = models.Sequential([
            # Block 1 - æ›´å¤š filters
            layers.Conv2D(48, (3, 3), padding='same', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(48, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 2
            layers.Conv2D(96, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(96, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 3
            layers.Conv2D(192, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Dense layers
            layers.Flatten(),
            layers.Dense(384, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(192, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ], name=f'{self.model_name}_Wide')
        
        return model
    
    def _build_deep_cnn(self):
        """æ›´æ·±çš„ CNN æ¶æ§‹ï¼ˆæ›´å¤šå±¤ï¼‰"""
        model = models.Sequential([
            # Block 1
            layers.Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(32, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(32, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 2
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(64, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Block 3
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.Conv2D(128, (3, 3), padding='same'),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Dense layers
            layers.Flatten(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(10, activation='softmax')
        ], name=f'{self.model_name}_Deep')
        
        return model
    
    def compile_model(self, learning_rate=0.001):
        """ç·¨è­¯æ¨¡å‹"""
        print(f"\nâš™ï¸  [{self.model_name}] ç·¨è­¯æ¨¡å‹...")
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("âœ“ æ¨¡å‹ç·¨è­¯å®Œæˆ")
    
    def train(self, epochs=30, batch_size=64):
        """è¨“ç·´æ¨¡å‹"""
        print("\n" + "="*60)
        print(f"ğŸš€ [{self.model_name}] é–‹å§‹è¨“ç·´")
        print("="*60)
        
        # è¨­å®šå›èª¿å‡½æ•¸
        early_stopping = callbacks.EarlyStopping(
            monitor='val_loss',
            patience=7,
            restore_best_weights=True,
            verbose=1
        )
        
        checkpoint = callbacks.ModelCheckpoint(
            f'{self.model_name}_best.keras',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=0
        )
        
        reduce_lr = callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=4,
            min_lr=1e-7,
            verbose=1
        )
        
        # è¨“ç·´
        start_time = datetime.now()
        
        self.history = self.model.fit(
            self.train_datagen.flow(self.X_train, self.y_train_categorical, 
                                   batch_size=batch_size),
            steps_per_epoch=len(self.X_train) // batch_size,
            epochs=epochs,
            validation_data=(self.X_val, self.y_val_categorical),
            callbacks=[early_stopping, checkpoint, reduce_lr],
            verbose=1
        )
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        print("\n" + "="*60)
        print(f"âœ… [{self.model_name}] è¨“ç·´å®Œæˆï¼è€—æ™‚ï¼š{training_time:.2f} ç§’")
        print("="*60)
        
        return self.history
    
    def evaluate(self):
        """è©•ä¼°æ¨¡å‹"""
        val_loss, val_accuracy = self.model.evaluate(
            self.X_val, self.y_val_categorical, 
            verbose=0
        )
        
        print(f"\nğŸ“Š [{self.model_name}] é©—è­‰é›†æº–ç¢ºç‡ï¼š{val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
        
        return val_accuracy
    
    def save_model(self):
        """å„²å­˜æ¨¡å‹"""
        filepath = f'{self.model_name}_final.keras'
        self.model.save(filepath)
        print(f"ğŸ’¾ [{self.model_name}] æ¨¡å‹å·²å„²å­˜ï¼š{filepath}")
        
        return filepath


# ==================== ä¸»ç¨‹å¼ï¼šè¨“ç·´ 5 å€‹æ¨¡å‹ ====================
if __name__ == "__main__":
    print("="*70)
    print("ğŸ¯ Ensemble è¨“ç·´è…³æœ¬ - è¨“ç·´å¤šå€‹ä¸åŒçš„æ¨¡å‹")
    print("="*70)
    
    # å®šç¾© 5 å€‹ä¸åŒçš„æ¨¡å‹é…ç½®
    model_configs = [
        {
            'model_id': 1,
            'random_seed': 42,
            'architecture': 'standard',
            'augmentation': 'standard',
            'learning_rate': 0.001,
            'batch_size': 64
        },
        {
            'model_id': 2,
            'random_seed': 123,
            'architecture': 'wide',
            'augmentation': 'standard',
            'learning_rate': 0.001,
            'batch_size': 64
        },
        {
            'model_id': 3,
            'random_seed': 456,
            'architecture': 'deep',
            'augmentation': 'mild',
            'learning_rate': 0.001,
            'batch_size': 64
        },
        {
            'model_id': 4,
            'random_seed': 789,
            'architecture': 'standard',
            'augmentation': 'aggressive',
            'learning_rate': 0.0008,
            'batch_size': 64
        },
        {
            'model_id': 5,
            'random_seed': 999,
            'architecture': 'wide',
            'augmentation': 'standard',
            'learning_rate': 0.0012,
            'batch_size': 48
        }
    ]
    
    # å„²å­˜æ¨¡å‹è³‡è¨Š
    models_info = []
    
    # è¨“ç·´æ¯å€‹æ¨¡å‹
    for i, config in enumerate(model_configs, 1):
        print("\n" + "="*70)
        print(f"ğŸ”„ é–‹å§‹è¨“ç·´æ¨¡å‹ {i}/{len(model_configs)}")
        print("="*70)
        print(f"é…ç½®ï¼š{config}")
        
        # åˆå§‹åŒ–è¨“ç·´å™¨
        trainer = EnsembleModelTrainer(
            model_id=config['model_id'],
            random_seed=config['random_seed']
        )
        
        # è¼‰å…¥è³‡æ–™
        trainer.load_preprocessed_data()
        
        # å»ºç«‹è³‡æ–™å¢å¼·
        trainer.create_data_augmentation(config['augmentation'])
        
        # å»ºç«‹æ¨¡å‹
        trainer.build_cnn_model(config['architecture'])
        
        # ç·¨è­¯æ¨¡å‹
        trainer.compile_model(config['learning_rate'])
        
        # è¨“ç·´æ¨¡å‹
        history = trainer.train(epochs=30, batch_size=config['batch_size'])
        
        # è©•ä¼°æ¨¡å‹
        val_accuracy = trainer.evaluate()
        
        # å„²å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        # è¨˜éŒ„æ¨¡å‹è³‡è¨Š
        model_info = {
            'model_id': config['model_id'],
            'model_name': trainer.model_name,
            'model_path': model_path,
            'val_accuracy': float(val_accuracy),
            'config': config
        }
        models_info.append(model_info)
        
        print(f"\nâœ… æ¨¡å‹ {i} è¨“ç·´å®Œæˆï¼é©—è­‰é›†æº–ç¢ºç‡ï¼š{val_accuracy:.4f}")
    
    # å„²å­˜æ‰€æœ‰æ¨¡å‹çš„è³‡è¨Š
    with open('ensemble_models_info.json', 'w') as f:
        json.dump(models_info, f, indent=2)
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰æ¨¡å‹è¨“ç·´å®Œæˆï¼")
    print("="*70)
    print("\nğŸ“Š æ¨¡å‹é©—è­‰é›†æº–ç¢ºç‡ç¸½è¦½ï¼š")
    for info in models_info:
        print(f"  {info['model_name']}: {info['val_accuracy']:.4f} ({info['val_accuracy']*100:.2f}%)")
    
    # è¨ˆç®—å¹³å‡æº–ç¢ºç‡
    avg_accuracy = np.mean([info['val_accuracy'] for info in models_info])
    print(f"\nğŸ“ˆ å¹³å‡é©—è­‰é›†æº–ç¢ºç‡ï¼š{avg_accuracy:.4f} ({avg_accuracy*100:.2f}%)")
    
    print("\nğŸ“„ æ¨¡å‹è³‡è¨Šå·²å„²å­˜åˆ°ï¼šensemble_models_info.json")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ ensemble_predict.py ä¾†çµ„åˆé€™äº›æ¨¡å‹çš„é æ¸¬")
    print("="*70)